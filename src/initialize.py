"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€æœ€åˆã®ç”»é¢èª­ã¿è¾¼ã¿æ™‚ã«ã®ã¿å®Ÿè¡Œã•ã‚Œã‚‹åˆæœŸåŒ–å‡¦ç†ãŒè¨˜è¿°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
from logging.handlers import TimedRotatingFileHandler
import os
import logging
import shutil
from uuid import uuid4
import sys
import unicodedata
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
import streamlit as st
from docx import Document
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
import constants as ct
from csv_employee_loader import EmployeeCSVLoader
from langchain_community.document_loaders import TextLoader
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document
import csv
from langchain_core.documents import Document
import csv
import glob
############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def initialize():
    """
    ç”»é¢èª­ã¿è¾¼ã¿æ™‚ã«å®Ÿè¡Œã™ã‚‹åˆæœŸåŒ–å‡¦ç†
    """
    # åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
    initialize_session_state()
    # ãƒ­ã‚°å‡ºåŠ›ç”¨ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã‚’ç”Ÿæˆ
    initialize_session_id()
    # ãƒ­ã‚°å‡ºåŠ›ã®è¨­å®š
    initialize_logger()
    # RAGã®Retrieverã‚’ä½œæˆ ï¼ˆretrieveræ§‹ç¯‰ã‚’åˆ‡ã‚Šå‡ºã—ï¼‰
    initialize_all_retrievers()


def initialize_logger():
    """
    ãƒ­ã‚°å‡ºåŠ›ã®è¨­å®š
    """
    # æŒ‡å®šã®ãƒ­ã‚°ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿ã€å­˜åœ¨ã—ãªã‘ã‚Œã°æ–°è¦ä½œæˆ
    os.makedirs(ct.LOG_DIR_PATH, exist_ok=True)
    
    # å¼•æ•°ã«æŒ‡å®šã—ãŸåå‰ã®ãƒ­ã‚¬ãƒ¼ï¼ˆãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’å–å¾—
    # å†åº¦åˆ¥ã®ç®‡æ‰€ã§å‘¼ã³å‡ºã—ãŸå ´åˆã€ã™ã§ã«åŒã˜åå‰ã®ãƒ­ã‚¬ãƒ¼ãŒå­˜åœ¨ã—ã¦ã„ã‚Œã°èª­ã¿è¾¼ã‚€
    logger = logging.getLogger(ct.LOGGER_NAME)

    # ã™ã§ã«ãƒ­ã‚¬ãƒ¼ã«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ã‚°ã®å‡ºåŠ›å…ˆã‚’åˆ¶å¾¡ã™ã‚‹ã‚‚ã®ï¼‰ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€åŒã˜ãƒ­ã‚°å‡ºåŠ›ãŒè¤‡æ•°å›è¡Œã‚ã‚Œãªã„ã‚ˆã†å‡¦ç†ã‚’ä¸­æ–­ã™ã‚‹
    if logger.hasHandlers():
        return

    # 1æ—¥å˜ä½ã§ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’ãƒªã‚»ãƒƒãƒˆã—ã€åˆ‡ã‚Šæ›¿ãˆã‚‹è¨­å®š
    log_handler = TimedRotatingFileHandler(
        os.path.join(ct.LOG_DIR_PATH, ct.LOG_FILE),
        when="D",
        encoding="utf8"
    )
    # å‡ºåŠ›ã™ã‚‹ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå®šç¾©
    # - ã€Œlevelnameã€: ãƒ­ã‚°ã®é‡è¦åº¦ï¼ˆINFO, WARNING, ERRORãªã©ï¼‰
    # - ã€Œasctimeã€: ãƒ­ã‚°ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼ˆã„ã¤è¨˜éŒ²ã•ã‚ŒãŸã‹ï¼‰
    # - ã€Œlinenoã€: ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œç•ªå·
    # - ã€ŒfuncNameã€: ãƒ­ã‚°ãŒå‡ºåŠ›ã•ã‚ŒãŸé–¢æ•°å
    # - ã€Œsession_idã€: ã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼ˆèª°ã®ã‚¢ãƒ—ãƒªæ“ä½œã‹åˆ†ã‹ã‚‹ã‚ˆã†ã«ï¼‰
    # - ã€Œmessageã€: ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    formatter = logging.Formatter(
        f"[%(levelname)s] %(asctime)s line %(lineno)s, in %(funcName)s, session_id={st.session_state.session_id}: %(message)s"
    )

    # å®šç¾©ã—ãŸãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®é©ç”¨
    log_handler.setFormatter(formatter)

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’ã€ŒINFOã€ã«è¨­å®š
    logger.setLevel(logging.INFO)

    # ä½œæˆã—ãŸãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ã‚°å‡ºåŠ›å…ˆã‚’åˆ¶å¾¡ã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’ã€
    # ãƒ­ã‚¬ãƒ¼ï¼ˆãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å®Ÿéš›ã«ç”Ÿæˆã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã«è¿½åŠ ã—ã¦ãƒ­ã‚°å‡ºåŠ›ã®æœ€çµ‚è¨­å®š
    logger.addHandler(log_handler)


def initialize_session_id():
    """
    ã‚»ãƒƒã‚·ãƒ§ãƒ³IDã®ä½œæˆ
    """
    if "session_id" not in st.session_state:
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—åˆ—ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³IDï¼‰ã‚’ã€ãƒ­ã‚°å‡ºåŠ›ç”¨ã«ä½œæˆ
        st.session_state.session_id = uuid4().hex


def initialize_all_retrievers():
    """
    ç¤¾å“¡åç°¿ç”¨ã¨å…¨ä½“ç”¨ã® retriever ã‚’æ§‹ç¯‰
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info("=== RetrieveråˆæœŸåŒ–é–‹å§‹ ===")

    if "employee_retriever" in st.session_state and "full_retriever" in st.session_state:
        return

    embeddings = OpenAIEmbeddings()
    text_splitter = CharacterTextSplitter(
        chunk_size=ct.CHUNK_SIZE,
        chunk_overlap=ct.CHUNK_OVERLAP,
        separator="\n"
    )

    # ğŸ”¹ ç¤¾å“¡åç°¿ retrieverï¼ˆåˆ†å‰²ã—ãªã„ï¼‹ãƒ•ã‚¡ã‚¤ãƒ«åè‡ªå‹•æ¤œå‡ºï¼‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
    employee_folder_path = os.path.join(ct.RAG_TOP_FOLDER_PATH, "ç¤¾å“¡ã«ã¤ã„ã¦")
    csv_files = glob.glob(os.path.join(employee_folder_path, "*.csv"))

    if not csv_files:
        raise FileNotFoundError("ç¤¾å“¡åç°¿ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    employee_csv_path = csv_files[0]
    csv_loader = EmployeeCSVLoader(file_path=employee_csv_path, encoding="utf-8-sig")
    employee_docs = csv_loader.load()
    print("[DEBUG] ç¤¾å“¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆsummaryå«ã‚€ï¼‰:", len(employee_docs))
    employee_count = 0
    found_hayashi = False
    departments = set()
    print("\n[DEBUG] ç¤¾å“¡ä¸€è¦§ï¼ˆéƒ¨ç½²ã¨åå‰ï¼‰:")
    for doc in employee_docs:
        if doc.metadata.get("type") != "employee":
            continue  # summaryãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯é™¤å¤–

        employee_count += 1
        content = doc.page_content
        dept = doc.metadata.get("department", "")
        departments.add(dept)

        print(f"- éƒ¨ç½²: {dept} / å†…å®¹: {content[:40]}...")  # çŸ­ãåˆ‡ã£ã¦è¡¨ç¤º
        
        if "æ—" in content and "åƒä»£" in content:
            found_hayashi = True
            print("âœ… æ— åƒä»£ã•ã‚“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼")

    print(f"\n[DEBUG] ç™»éŒ²ã•ã‚ŒãŸç¤¾å“¡æ•°ï¼ˆsummaryé™¤ãï¼‰: {employee_count}")
    print(f"[DEBUG] ç™»éŒ²ã•ã‚ŒãŸéƒ¨ç½²ä¸€è¦§: {sorted(departments)}")

    if not found_hayashi:
        print("âŒ æ— åƒä»£ã•ã‚“ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ employee_docs ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    for doc in employee_docs:
        doc.metadata["category"] = "employee"

    employee_db = Chroma.from_documents(
        documents=employee_docs,
        embedding=embeddings,
        collection_metadata={"category": "employee"}
    )

    st.session_state.employee_retriever = employee_db.as_retriever(
        search_kwargs={
            "k": 100,
            "filter": {"category": "employee"}
        }
    )

    # ğŸ”¸ å…¨ä½“ retrieverï¼ˆå¾“æ¥é€šã‚Šåˆ†å‰²ã‚ã‚Šï¼‰
    full_docs = load_data_sources()
    splitted_docs = text_splitter.split_documents(full_docs)
    full_db = Chroma.from_documents(splitted_docs, embedding=embeddings)
    st.session_state.full_retriever = full_db.as_retriever(search_kwargs={"k": ct.NUM_RELATED_DOCUMENTS})

    # âœ… ãƒ‡ãƒãƒƒã‚°ç”¨ï¼ˆå‰Šé™¤ã—ã¦ã‚‚OKï¼‰
    for doc in employee_docs:
        print("----")
        print(doc.page_content)


def initialize_session_state():
    """
    åˆæœŸåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç”¨æ„
    """
    if "messages" not in st.session_state:
        # ã€Œè¡¨ç¤ºç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.messages = []
        # ã€ŒLLMã¨ã®ã‚„ã‚Šã¨ã‚Šç”¨ã€ã®ä¼šè©±ãƒ­ã‚°ã‚’é †æ¬¡æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆã‚’ç”¨æ„
        st.session_state.chat_history = []


def get_loader(file_path, ext):
    """æ‹¡å¼µå­ã«å¿œã˜ãŸé©åˆ‡ãªãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’å–å¾—ã™ã‚‹"""
    if ext == ".csv":
        return EmployeeCSVLoader(file_path, encoding="utf-8-sig")
    
    loader_class = ct.SUPPORTED_EXTENSIONS.get(ext)
    if loader_class:
        # TextLoaderã®å ´åˆã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æŒ‡å®š
        if loader_class == TextLoader:
            return loader_class(file_path, encoding="utf-8")
        return loader_class(file_path)
    
    return None


def load_documents_from_path(path):
    """æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å†å¸°çš„ã«èª­ã¿è¾¼ã‚€"""
    documents = []
    logger = logging.getLogger(ct.LOGGER_NAME)
    logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¢ç´¢é–‹å§‹: {path}")
    
    for root, _, files in os.walk(path):
        for file in files:
            file_path = os.path.join(root, file)
            file_ext = os.path.splitext(file)[1].lower()
            
            loader = get_loader(file_path, file_ext)
            if loader:
                try:
                    docs = loader.load()
                    documents.extend(docs)
                    logger.info(f"èª­ã¿è¾¼ã¿æˆåŠŸ: {file_path} ({len(docs)}ä»¶)")
                except Exception as e:
                    logger.error(f"èª­ã¿è¾¼ã¿å¤±æ•—: {file_path}, ã‚¨ãƒ©ãƒ¼: {e}")
    return documents


def load_data_sources():
    """
    RAGã®å‚ç…§å…ˆã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
    """
    logger = logging.getLogger(ct.LOGGER_NAME)
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€
    docs_all = load_documents_from_path(ct.RAG_TOP_FOLDER_PATH)

    # 2. Webãƒ™ãƒ¼ã‚¹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€
    web_docs_all = []
    if hasattr(ct, 'WEB_URL_LOAD_TARGETS') and ct.WEB_URL_LOAD_TARGETS:
        logger.info(f"Webèª­ã¿è¾¼ã¿é–‹å§‹: {len(ct.WEB_URL_LOAD_TARGETS)}ä»¶ã®URL")
        for web_url in ct.WEB_URL_LOAD_TARGETS:
            try:
                loader = WebBaseLoader(web_url)
                web_docs = loader.load()
                web_docs_all.extend(web_docs)
                logger.info(f"Webèª­ã¿è¾¼ã¿æˆåŠŸ: {web_url}")
            except Exception as e:
                logger.error(f"Webèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {web_url}: {e}")
    else:
        logger.info("WEB_URL_LOAD_TARGETSãŒæœªè¨­å®šã¾ãŸã¯ç©ºã®ãŸã‚ã€Webèª­ã¿è¾¼ã¿ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã¨Webã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
    docs_all.extend(web_docs_all)
    logger.info(f"ç·èª­ã¿è¾¼ã¿å®Œäº†: åˆè¨ˆ{len(docs_all)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")

    return docs_all


def adjust_string(s):
    """
    Windowsç’°å¢ƒã§RAGãŒæ­£å¸¸å‹•ä½œã™ã‚‹ã‚ˆã†èª¿æ•´
    
    Args:
        s: èª¿æ•´ã‚’è¡Œã†æ–‡å­—åˆ—
    
    Returns:
        èª¿æ•´ã‚’è¡Œã£ãŸæ–‡å­—åˆ—
    """
    # èª¿æ•´å¯¾è±¡ã¯æ–‡å­—åˆ—ã®ã¿
    if type(s) is not str:
        return s

    # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
    if sys.platform.startswith("win"):
        s = unicodedata.normalize('NFC', s)
        s = s.encode("cp932", "ignore").decode("cp932")
        return s
    
    # OSãŒWindowsä»¥å¤–ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    return s


def initialize_rag_system():
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
    try:
        logger.info("=== RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹ ===")
        logger.info(f"ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {os.getcwd()}")
        logger.info(f"DATA_DIR: {ct.DATA_DIR}")
        
        # ğŸ”¥ å…¨ã¦ã®DBå‰Šé™¤ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ä»˜ãï¼‰
        db_paths = [
            os.path.join(ct.DATA_DIR, "vectorstore"),
            os.path.join(os.path.dirname(__file__), "chroma_db"),
            os.path.join(os.path.dirname(__file__), "..", "chroma_db"),
            "chroma_db",
            ".chroma"
        ]
        
        for db_path in db_paths:
            abs_path = os.path.abspath(db_path)
            logger.info(f"ãƒã‚§ãƒƒã‚¯ä¸­: {abs_path}")
            if os.path.exists(abs_path):
                shutil.rmtree(abs_path)
                logger.info(f"âœ… DBå‰Šé™¤: {abs_path}")
            else:
                logger.info(f"â­ï¸ å­˜åœ¨ã—ãªã„: {abs_path}")

        # CSVãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªï¼ˆè©³ç´°ãƒ­ã‚°ï¼‰
        csv_path = os.path.join(ct.DATA_DIR, "ç¤¾å“¡ã«ã¤ã„ã¦", "ç¤¾å“¡åç°¿.csv")
        abs_csv_path = os.path.abspath(csv_path)
        logger.info(f"CSVãƒ‘ã‚¹: {abs_csv_path}")
        logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨: {os.path.exists(abs_csv_path)}")
        
        if os.path.exists(abs_csv_path):
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚‚ç¢ºèª
            file_size = os.path.getsize(abs_csv_path)
            logger.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size} bytes")

            csv_loader = EmployeeCSVLoader(csv_path)
            csv_documents = csv_loader.load()
            logger.info(f"CSVæ–‡æ›¸æ•°: {len(csv_documents)}")
            
            # CSVæ–‡æ›¸ã‚’æœ€åˆã«è¿½åŠ 
            all_documents = csv_documents
        else:
            logger.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
            all_documents = []

        # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿
        docs_all = load_data_sources()
        all_documents.extend(docs_all)

        # è©³ç´°ãªèª­ã¿è¾¼ã¿çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"ç·èª­ã¿è¾¼ã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(all_documents)}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®èª­ã¿è¾¼ã¿çŠ¶æ³ã‚’ç¢ºèª
        file_counts = {}
        for doc in all_documents:
            source = doc.metadata.get("source", "Unknown")
            file_counts[source] = file_counts.get(source, 0) + 1
        
        for file_path, count in file_counts.items():
            logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿è©³ç´°: {file_path} â†’ {count}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
        
        # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ç¢ºèª
        csv_docs = [doc for doc in all_documents if ".csv" in doc.metadata.get("source", "")]
        logger.info(f"CSVã‹ã‚‰èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(csv_docs)}")
        
        # äººäº‹éƒ¨ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ç¢ºèª
        hr_docs = [doc for doc in all_documents if "äººäº‹éƒ¨" in doc.page_content]
        logger.info(f"äººäº‹éƒ¨ã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(hr_docs)}")
        
        # OSãŒWindowsã®å ´åˆã€Unicodeæ­£è¦åŒ–ã¨ã€cp932ï¼ˆWindowsç”¨ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰ï¼‰ã§è¡¨ç¾ã§ããªã„æ–‡å­—ã‚’é™¤å»
        for doc in all_documents:
            doc.page_content = adjust_string(doc.page_content)
            for key in doc.metadata:
                doc.metadata[key] = adjust_string(doc.metadata[key])
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç”¨æ„
        embeddings = OpenAIEmbeddings()
        
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ç”¨ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        text_splitter = CharacterTextSplitter(
            chunk_size=ct.CHUNK_SIZE,
            chunk_overlap=ct.CHUNK_OVERLAP,
            separator="\n"
        )

        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’å®Ÿæ–½
        splitted_docs = text_splitter.split_documents(all_documents)
        
        logger.info(f"ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¾Œã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(splitted_docs)}")

        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ä½œæˆï¼ˆæ°¸ç¶šåŒ–è¨­å®šã‚’è¿½åŠ ï¼‰
        db = Chroma.from_documents(
            documents=splitted_docs, 
            embedding=embeddings,
            persist_directory=src_db_path
        )
        
        logger.info("æ–°ã—ã„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸ")

        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’æ¤œç´¢ã™ã‚‹Retrieverã®ä½œæˆ
        st.session_state.retriever = db.as_retriever(search_kwargs={"k": ct.NUM_RELATED_DOCUMENTS})
        
        logger.info("RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    except Exception as e:
        logger.error(f"RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")